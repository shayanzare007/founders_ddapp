#! /usr/bin/env python
#New version of features, optimized by deepdive

import sys
import os
import ddlib     # DeepDive python utility

ARR_DELIM = '~^~'

# Load keyword dictionaries using ddlib, for domain-specific features
# Words in "founders" dictionary are indicative of companies founded
# Words in "non_founders" dictionary are indicative of non companies founded
BASE_DIR = os.path.dirname(os.path.realpath(__file__))
ddlib.load_dictionary(BASE_DIR + "/dicts/founder.txt", dict_id="founder")
ddlib.load_dictionary(BASE_DIR + "/dicts/non_founder.txt", dict_id="non_founder")

# For each input tuple
for row in sys.stdin:
  parts = row.strip().split('\t')

  # Get all fields from a row
  words = parts[0].split(ARR_DELIM)
  lemmas = parts[1].split(ARR_DELIM)
  poses = parts[2].split(ARR_DELIM)
  dependencies = parts[3].split(ARR_DELIM)
  ners = parts[4].split(ARR_DELIM)
  relation_id = parts[5]
  p1_start, p1_length, p2_start, p2_length = [int(x) for x in parts[6:]]

  # Skip lines with empty dependency paths
  if len(dependencies) == 0:
    print >>sys.stderr, str(relation_id) + '\t' + 'DEP_PATH_EMPTY'
    continue

  # Get a sentence from ddlib -- array of "Word" objects
  try:
    sentence = ddlib.get_sentence(
        [0, ] * len(words),  [0, ] * len(words), words, lemmas, poses,
        dependencies, ners)
  except:
    print >>sys.stderr, dependencies
    continue

  # Create two spans of person mentions
  span1 = ddlib.Span(begin_word_id=p1_start, length=p1_length)
  span2 = ddlib.Span(begin_word_id=p2_start, length=p2_length)

  # Features for this pair come in here
  features = set()

  # Get generic features generated by ddlib
  for feature in ddlib.get_generic_features_relation(sentence, span1, span2):
    features.add(feature)
  for feature in features:
    print str(relation_id) + '\t' + feature

'''
#Old version of features
#! /usr/bin/env python

import sys
import ddlib     # DeepDive python utility

ARR_DELIM = '~^~'
nbWordsBetweenPeopleCompanyConsidered = 5

# For each input tuple
for row in sys.stdin:
  parts = row.strip().split('\t')
  if len(parts) != 7: 
    print >>sys.stderr, 'Failed to parse row:', row
    continue
  
  # Get all fields from a row
  words = parts[0].split(ARR_DELIM)
  relation_id = parts[1]
  p2_text = parts[2]
  p1_start, p1_length, p2_start, p2_length = [int(x) for x in parts[3:]]

  # Unpack input into tuples.
  span1 = ddlib.Span(begin_word_id=p1_start, length=p1_length)
  span2 = ddlib.Span(begin_word_id=p2_start, length=p2_length)

  # Features for this pair come in here
  features = set()
  
  # Feature 1: Bag of words between the two phrases
  words_between = ddlib.tokens_between_spans(words, span1, span2)
  count = 1
  for word in words_between.elements:
    if count < nbWordsBetweenPeopleCompanyConsidered:
      features.add("word_between=" + word)
    count +=1
    

  # Feature 2: Number of words between the two phrases
  features.add("num_words_between=%s" % len(words_between.elements))

  # Feature 3: Is the last name of the founder included in the name of the company?
  last_word_left = ddlib.materialize_span(words, span1)[-1]
  if (last_word_left in p2_text):
    features.add("potential_last_name_match")

  for feature in features:  
    print str(relation_id) + '\t' + feature 
'''
